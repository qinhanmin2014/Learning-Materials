{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is an implementation of Net2Net experiment with MNIST in\n",
    "'Net2Net: Accelerating Learning via Knowledge Transfer'\n",
    "by Tianqi Chen, Ian Goodfellow, and Jonathon Shlens\n",
    "\n",
    "arXiv:1511.05641v4 [cs.LG] 23 Apr 2016\n",
    "http://arxiv.org/abs/1511.05641\n",
    "\n",
    "# Notes\n",
    "\n",
    "- What:\n",
    "  + Net2Net is a group of methods to transfer knowledge from a teacher neural\n",
    "    net to a student net,so that the student net can be trained faster than\n",
    "    from scratch.\n",
    "  + The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet\n",
    "    and Net2DeeperNet.\n",
    "  + Net2WiderNet replaces a model with an equivalent wider model that has\n",
    "    more units in each hidden layer.\n",
    "  + Net2DeeperNet replaces a model with an equivalent deeper model.\n",
    "  + Both are based on the idea of 'function-preserving transformations of\n",
    "    neural nets'.\n",
    "- Why:\n",
    "  + Enable fast exploration of multiple neural nets in experimentation and\n",
    "    design process,by creating a series of wider and deeper models with\n",
    "    transferable knowledge.\n",
    "  + Enable 'lifelong learning system' by gradually adjusting model complexity\n",
    "    to data availability,and reusing transferable knowledge.\n",
    "\n",
    "# Experiments\n",
    "\n",
    "- Teacher model: a basic CNN model trained on MNIST for 3 epochs.\n",
    "- Net2WiderNet experiment:\n",
    "  + Student model has a wider Conv2D layer and a wider FC layer.\n",
    "  + Comparison of 'random-padding' vs 'net2wider' weight initialization.\n",
    "  + With both methods, after 1 epoch, student model should perform as well as\n",
    "    teacher model, but 'net2wider' is slightly better.\n",
    "- Net2DeeperNet experiment:\n",
    "  + Student model has an extra Conv2D layer and an extra FC layer.\n",
    "  + Comparison of 'random-init' vs 'net2deeper' weight initialization.\n",
    "  + After 1 epoch, performance of 'net2deeper' is better than 'random-init'.\n",
    "- Hyper-parameters:\n",
    "  + SGD with momentum=0.9 is used for training teacher and student models.\n",
    "  + Learning rate adjustment: it's suggested to reduce learning rate\n",
    "    to 1/10 for student model.\n",
    "  + Addition of noise in 'net2wider' is used to break weight symmetry\n",
    "    and thus enable full capacity of student models. It is optional\n",
    "    when a Dropout layer is used.\n",
    "\n",
    "# Results\n",
    "\n",
    "- Tested with TF backend and 'channels_last' image_data_format.\n",
    "- Running on GPU GeForce GTX Titan X Maxwell\n",
    "- Performance Comparisons - validation loss values during first 3 epochs:\n",
    "\n",
    "Teacher model ...\n",
    "(0) teacher_model:            0.0537   0.0354   0.0356\n",
    "\n",
    "Experiment of Net2WiderNet ...\n",
    "(1) wider_random_pad:          0.0320   0.0317   0.0289\n",
    "(2) wider_net2wider:           0.0271   0.0274   0.0270\n",
    "\n",
    "Experiment of Net2DeeperNet ...\n",
    "(3) deeper_random_init:        0.0682   0.0506   0.0468\n",
    "(4) deeper_net2deeper:         0.0292   0.0294   0.0286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.2\n",
      "2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)  # image shape\n",
    "num_classes = 10  # number of classes\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data...\n",
      "x_train shape: (60000, 28, 28, 1) y_train shape: (60000, 10)\n",
      "x_test shape: (10000, 28, 28, 1) y_test shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load and pre-process data\n",
    "def preprocess_input(x):\n",
    "    return x.astype('float32').reshape((-1,) + input_shape) / 255\n",
    "\n",
    "\n",
    "def preprocess_output(y):\n",
    "    return keras.utils.to_categorical(y)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = map(preprocess_input, [x_train, x_test])\n",
    "y_train, y_test = map(preprocess_output, [y_train, y_test])\n",
    "print('Loading MNIST data...')\n",
    "print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge transfer algorithms\n",
    "def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n",
    "    '''Get initial weights for a wider conv2d layer with a bigger filters,\n",
    "    by 'random-padding' or 'net2wider'.\n",
    "\n",
    "    # Arguments\n",
    "        teacher_w1: `weight` of conv2d layer to become wider,\n",
    "          of shape (kh1, kw1, filters1, num_channel1)\n",
    "        teacher_b1: `bias` of conv2d layer to become wider,\n",
    "          of shape (filters1, )\n",
    "        teacher_w2: `weight` of next connected conv2d layer,\n",
    "          of shape (kh2, kw2, filters2, num_channel2)\n",
    "        new_width: new `filters` for the wider conv2d layer\n",
    "        init: initialization algorithm for new weights,\n",
    "          either 'random-pad' or 'net2wider'\n",
    "    '''\n",
    "    assert teacher_w1.shape[3] == teacher_w2.shape[2], (\n",
    "        'successive layers from teacher model should have compatible shapes')\n",
    "    assert teacher_w1.shape[3] == teacher_b1.shape[0], (\n",
    "        'weight and bias from same layer should have compatible shapes')\n",
    "    assert new_width > teacher_w1.shape[3], (\n",
    "        'new width (filters) should be bigger than the existing one')\n",
    "\n",
    "    n = new_width - teacher_w1.shape[3]\n",
    "    if init == 'random-pad':\n",
    "        new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,))\n",
    "        new_b1 = np.ones(n) * 0.1\n",
    "        new_w2 = np.random.normal(\n",
    "            0, 0.1,\n",
    "            size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3]))\n",
    "    elif init == 'net2wider':\n",
    "        index = np.random.randint(teacher_w1.shape[3], size=n)\n",
    "        factors = np.bincount(index)[index] + 1.\n",
    "        new_w1 = teacher_w1[:, :, :, index]\n",
    "        new_b1 = teacher_b1[index]\n",
    "        new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1))\n",
    "    else:\n",
    "        raise ValueError('Unsupported weight initializer: %s' % init)\n",
    "\n",
    "    student_w1 = np.concatenate((teacher_w1, new_w1), axis=3)\n",
    "    if init == 'random-pad':\n",
    "        student_w2 = np.concatenate((teacher_w2, new_w2), axis=2)\n",
    "    elif init == 'net2wider':\n",
    "        # add small noise to break symmetry, so that student model will have\n",
    "        # full capacity later\n",
    "        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n",
    "        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2)\n",
    "        student_w2[:, :, index, :] = new_w2\n",
    "    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n",
    "\n",
    "    return student_w1, student_b1, student_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n",
    "    '''Get initial weights for a wider fully connected (dense) layer\n",
    "       with a bigger nout, by 'random-padding' or 'net2wider'.\n",
    "\n",
    "    # Arguments\n",
    "        teacher_w1: `weight` of fc layer to become wider,\n",
    "          of shape (nin1, nout1)\n",
    "        teacher_b1: `bias` of fc layer to become wider,\n",
    "          of shape (nout1, )\n",
    "        teacher_w2: `weight` of next connected fc layer,\n",
    "          of shape (nin2, nout2)\n",
    "        new_width: new `nout` for the wider fc layer\n",
    "        init: initialization algorithm for new weights,\n",
    "          either 'random-pad' or 'net2wider'\n",
    "    '''\n",
    "    assert teacher_w1.shape[1] == teacher_w2.shape[0], (\n",
    "        'successive layers from teacher model should have compatible shapes')\n",
    "    assert teacher_w1.shape[1] == teacher_b1.shape[0], (\n",
    "        'weight and bias from same layer should have compatible shapes')\n",
    "    assert new_width > teacher_w1.shape[1], (\n",
    "        'new width (nout) should be bigger than the existing one')\n",
    "\n",
    "    n = new_width - teacher_w1.shape[1]\n",
    "    if init == 'random-pad':\n",
    "        new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n))\n",
    "        new_b1 = np.ones(n) * 0.1\n",
    "        new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1]))\n",
    "    elif init == 'net2wider':\n",
    "        index = np.random.randint(teacher_w1.shape[1], size=n)\n",
    "        factors = np.bincount(index)[index] + 1.\n",
    "        new_w1 = teacher_w1[:, index]\n",
    "        new_b1 = teacher_b1[index]\n",
    "        new_w2 = teacher_w2[index, :] / factors[:, np.newaxis]\n",
    "    else:\n",
    "        raise ValueError('Unsupported weight initializer: %s' % init)\n",
    "\n",
    "    student_w1 = np.concatenate((teacher_w1, new_w1), axis=1)\n",
    "    if init == 'random-pad':\n",
    "        student_w2 = np.concatenate((teacher_w2, new_w2), axis=0)\n",
    "    elif init == 'net2wider':\n",
    "        # add small noise to break symmetry, so that student model will have\n",
    "        # full capacity later\n",
    "        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n",
    "        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0)\n",
    "        student_w2[index, :] = new_w2\n",
    "    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n",
    "\n",
    "    return student_w1, student_b1, student_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeper2net_conv2d(teacher_w):\n",
    "    '''Get initial weights for a deeper conv2d layer by net2deeper'.\n",
    "\n",
    "    # Arguments\n",
    "        teacher_w: `weight` of previous conv2d layer,\n",
    "          of shape (kh, kw, num_channel, filters)\n",
    "    '''\n",
    "    kh, kw, num_channel, filters = teacher_w.shape\n",
    "    student_w = np.zeros_like(teacher_w)\n",
    "    for i in range(filters):\n",
    "        student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1.\n",
    "    student_b = np.zeros(filters)\n",
    "    return student_w, student_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(teacher_model, student_model, layer_names):\n",
    "    '''Copy weights from teacher_model to student_model,\n",
    "     for layers with names listed in layer_names\n",
    "    '''\n",
    "    for name in layer_names:\n",
    "        weights = teacher_model.get_layer(name=name).get_weights()\n",
    "        student_model.get_layer(name=name).set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to construct teacher_model and student_models\n",
    "def make_teacher_model(x_train, y_train,\n",
    "                       x_test, y_test,\n",
    "                       epochs):\n",
    "    '''Train and benchmark performance of a simple CNN.\n",
    "    (0) Teacher model\n",
    "    '''\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, 3, input_shape=input_shape,\n",
    "                            padding='same', name='conv1'))\n",
    "    model.add(layers.MaxPooling2D(2, name='pool1'))\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', name='conv2'))\n",
    "    model.add(layers.MaxPooling2D(2, name='pool2'))\n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    model.add(layers.Dense(64, activation='relu', name='fc1'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', name='fc2'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wider_student_model(teacher_model,\n",
    "                             x_train, y_train,\n",
    "                             x_test, y_test,\n",
    "                             init, epochs):\n",
    "    '''Train a wider student model based on teacher_model,\n",
    "       with either 'random-pad' (baseline) or 'net2wider'\n",
    "    '''\n",
    "    new_conv1_width = 128\n",
    "    new_fc1_width = 128\n",
    "\n",
    "    model = models.Sequential()\n",
    "    # a wider conv1 compared to teacher_model\n",
    "    model.add(layers.Conv2D(new_conv1_width, 3, input_shape=input_shape,\n",
    "                            padding='same', name='conv1'))\n",
    "    model.add(layers.MaxPooling2D(2, name='pool1'))\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', name='conv2'))\n",
    "    model.add(layers.MaxPooling2D(2, name='pool2'))\n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    # a wider fc1 compared to teacher model\n",
    "    model.add(layers.Dense(new_fc1_width, activation='relu', name='fc1'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', name='fc2'))\n",
    "\n",
    "    # The weights for other layers need to be copied from teacher_model\n",
    "    # to student_model, except for widened layers\n",
    "    # and their immediate downstreams, which will be initialized separately.\n",
    "    # For this example there are no other layers that need to be copied.\n",
    "\n",
    "    w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights()\n",
    "    w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights()\n",
    "    new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d(\n",
    "        w_conv1, b_conv1, w_conv2, new_conv1_width, init)\n",
    "    model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1])\n",
    "    model.get_layer('conv2').set_weights([new_w_conv2, b_conv2])\n",
    "\n",
    "    w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights()\n",
    "    w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights()\n",
    "    new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc(\n",
    "        w_fc1, b_fc1, w_fc2, new_fc1_width, init)\n",
    "    model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1])\n",
    "    model.get_layer('fc2').set_weights([new_w_fc2, b_fc2])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deeper_student_model(teacher_model,\n",
    "                              x_train, y_train,\n",
    "                              x_test, y_test,\n",
    "                              init, epochs):\n",
    "    '''Train a deeper student model based on teacher_model,\n",
    "       with either 'random-init' (baseline) or 'net2deeper'\n",
    "    '''\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, 3, input_shape=input_shape,\n",
    "                            padding='same', name='conv1'))\n",
    "    model.add(layers.MaxPooling2D(2, name='pool1'))\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', name='conv2'))\n",
    "    # add another conv2d layer to make original conv2 deeper\n",
    "    if init == 'net2deeper':\n",
    "        prev_w, _ = model.get_layer('conv2').get_weights()\n",
    "        new_weights = deeper2net_conv2d(prev_w)\n",
    "        model.add(layers.Conv2D(64, 3, padding='same',\n",
    "                                name='conv2-deeper', weights=new_weights))\n",
    "    elif init == 'random-init':\n",
    "        model.add(layers.Conv2D(64, 3, padding='same', name='conv2-deeper'))\n",
    "    else:\n",
    "        raise ValueError('Unsupported weight initializer: %s' % init)\n",
    "    model.add(layers.MaxPooling2D(2, name='pool2'))\n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    model.add(layers.Dense(64, activation='relu', name='fc1'))\n",
    "    # add another fc layer to make original fc1 deeper\n",
    "    if init == 'net2deeper':\n",
    "        # net2deeper for fc layer with relu, is just an identity initializer\n",
    "        model.add(layers.Dense(64, kernel_initializer='identity',\n",
    "                               activation='relu', name='fc1-deeper'))\n",
    "    elif init == 'random-init':\n",
    "        model.add(layers.Dense(64, activation='relu', name='fc1-deeper'))\n",
    "    else:\n",
    "        raise ValueError('Unsupported weight initializer: %s' % init)\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', name='fc2'))\n",
    "\n",
    "    # copy weights for other layers\n",
    "    copy_weights(teacher_model, model, layer_names=[\n",
    "                 'conv1', 'conv2', 'fc1', 'fc2'])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments setup\n",
    "def net2wider_experiment():\n",
    "    '''Benchmark performances of\n",
    "    (1) a wider student model with `random_pad` initializer\n",
    "    (2) a wider student model with `Net2WiderNet` initializer\n",
    "    '''\n",
    "    print('\\nExperiment of Net2WiderNet ...')\n",
    "\n",
    "    print('\\n(1) building wider student model by random padding ...')\n",
    "    make_wider_student_model(teacher_model,\n",
    "                             x_train, y_train,\n",
    "                             x_test, y_test,\n",
    "                             init='random-pad',\n",
    "                             epochs=epochs)\n",
    "    print('\\n(2) building wider student model by net2wider ...')\n",
    "    make_wider_student_model(teacher_model,\n",
    "                             x_train, y_train,\n",
    "                             x_test, y_test,\n",
    "                             init='net2wider',\n",
    "                             epochs=epochs)\n",
    "\n",
    "\n",
    "def net2deeper_experiment():\n",
    "    '''Benchmark performances of\n",
    "    (3) a deeper student model with `random_init` initializer\n",
    "    (4) a deeper student model with `Net2DeeperNet` initializer\n",
    "    '''\n",
    "    print('\\nExperiment of Net2DeeperNet ...')\n",
    "\n",
    "    print('\\n(3) building deeper student model by random init ...')\n",
    "    make_deeper_student_model(teacher_model,\n",
    "                              x_train, y_train,\n",
    "                              x_test, y_test,\n",
    "                              init='random-init',\n",
    "                              epochs=epochs)\n",
    "    print('\\n(4) building deeper student model by net2deeper ...')\n",
    "    make_deeper_student_model(teacher_model,\n",
    "                              x_train, y_train,\n",
    "                              x_test, y_test,\n",
    "                              init='net2deeper',\n",
    "                              epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0) building teacher model ...\n",
      "WARNING:tensorflow:From /home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/sunxin/miniconda3/envs/qinhanmin-test/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.1770 - acc: 0.9458 - val_loss: 0.0524 - val_acc: 0.9837\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0502 - acc: 0.9844 - val_loss: 0.0478 - val_acc: 0.9846\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0341 - acc: 0.9892 - val_loss: 0.0392 - val_acc: 0.9874\n"
     ]
    }
   ],
   "source": [
    "print('\\n(0) building teacher model ...')\n",
    "teacher_model = make_teacher_model(x_train, y_train,\n",
    "                                   x_test, y_test,\n",
    "                                   epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment of Net2WiderNet ...\n",
      "\n",
      "(1) building wider student model by random padding ...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0224 - acc: 0.9925 - val_loss: 0.0323 - val_acc: 0.9898\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0148 - acc: 0.9956 - val_loss: 0.0333 - val_acc: 0.9889\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0123 - acc: 0.9966 - val_loss: 0.0323 - val_acc: 0.9891\n",
      "\n",
      "(2) building wider student model by net2wider ...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0160 - acc: 0.9955 - val_loss: 0.0283 - val_acc: 0.9906\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0281 - val_acc: 0.9906\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0118 - acc: 0.9968 - val_loss: 0.0283 - val_acc: 0.9906\n",
      "\n",
      "Experiment of Net2DeeperNet ...\n",
      "\n",
      "(3) building deeper student model by random init ...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.1581 - acc: 0.9528 - val_loss: 0.0714 - val_acc: 0.9761\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0559 - acc: 0.9827 - val_loss: 0.0459 - val_acc: 0.9852\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0410 - acc: 0.9873 - val_loss: 0.0409 - val_acc: 0.9867\n",
      "\n",
      "(4) building deeper student model by net2deeper ...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0167 - acc: 0.9950 - val_loss: 0.0287 - val_acc: 0.9905\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.0298 - val_acc: 0.9903\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0317 - val_acc: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# run the experiments\n",
    "net2wider_experiment()\n",
    "net2deeper_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qinhanmin-test",
   "language": "python",
   "name": "qinhanmin-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
